# -*- coding: utf-8 -*-
"""JobRecommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oOEcAycMDZp4fxQYfz4dctKI9OiL3jdo

# Job Recommendation System
---
### By. [Sinta Siti Nuriah](https://www.linkedin.com/in/sintasitinuriah/)
![job](https://www.northyorks.gov.uk/sites/default/files/2024-04/jobsearchlandingimage.jpg)

# Data Understanding

## a. Data Loading
"""

!pip install -q kaggle
!pip install lightfm

import re
import ast
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score

from lightfm import LightFM
from lightfm.cross_validation import random_train_test_split
from lightfm.data import Dataset
from lightfm.evaluation import precision_at_k, auc_score, recall_at_k

from google.colab import drive
drive.mount('/content/drive')

!mkdir ~/.kaggle
!cp /content/drive/MyDrive/Colab\ Notebooks/MLT/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d lastman0800/job-recomendation-dataset

!unzip /content/job-recomendation-dataset.zip

df = pd.read_csv('/content/Job Datsset.csv')

df

df.shape

"""Data ini memiliki 100.000 data dengan 6 kolom. kolom-kolom yang dimaksud adalah sebagai berikut; `User_ID`, `Job_ID`, `User_Skills`, `Job_Requirements`, `Match_Score` dan `Recommended`. Data yang dimiliki cukup banyak dan memungkinan untuk membuat job recommendation system lebih optimal.

## b. Deskripsi Variabel
Variabel-variabel pada dataset job rekomendai adalah sebagai berikut:
- `User_ID`: ID unik untuk setiap pengguna dalam dataset.
- `Job_ID`: ID unik untuk setiap pekerjaan yang tercantum dalam dataset.
- `User_Skills`: Daftar keterampilan yang dimiliki oleh pengguna.
- `Job_Requirements`: Daftar keterampilan yang dibutuhkan untuk setiap pekerjaan
- `Match_Score`: Skor kecocokan antara keterampilan pengguna dan kebutuhan pekerjaan. Semakin tinggi, semakin cocok.
- `Recommended`:  Indikator biner: 1 jika pekerjaan direkomendasikan, 0 jika tidak.

### c. Jumlah Data pada Dataset
"""

for i, (nama_kolom, jumlah) in enumerate(df.count().items(), 1):
  print(f'Kolom {i}: {nama_kolom} - Berjumlah: {jumlah}')

df.info()

"""Dari hasil diatas terdapat:
- Total tabel yang ada pada dataset tersebut adalah 6 kolom
- tipe data `object` ada 2 kolom yaitu `User_Skills` dan `Job_Requirements`
- tipe data `int64` ada 3 kolom yaitu `User_ID`, `Job_ID`, dan `Recommended`
- tipe data `float64` hanya satu yaitu `Match_Score`
"""

df.describe()

"""Fungsi describe() memberikan informasi statistik pada masing-masing kolom, antara lain:

* Count adalah jumlah sampel pada data.
* Mean adalah nilai rata-rata.
* Std adalah standar deviasi.
* Min yaitu nilai minimum setiap kolom.
* Max adalah nilai maksimum.
---
Kuartil adalah nilai yang menandai batas interval dalam empat bagian sebaran yang sama.
* 25% adalah kuartil pertama.
* 50% adalah kuartil kedua, atau biasa juga disebut median (nilai tengah).
* 75% adalah kuartil ketiga.

### d. Kondisi Dataset
"""

df.isnull().sum()

"""Dataset ini tidak memiliki nilai kosong"""

df.duplicated().sum()

"""Data ini juga tidak memiliki nilai yang duplikat"""

# total outlier
Q1 = df.select_dtypes(include=np.number).quantile(0.25)
Q3 = df.select_dtypes(include=np.number).quantile(0.75)
IQR = Q3 - Q1
print(IQR)

"""Dataset ini memiliki otlier pada fitur `User_ID`, `Job_ID`, `Match_Score` dan `Recommended`

### Tanda
Fitur yang dimanfaatkan untuk membangun rekomendasi sederhana adalah `User_ID`, `Job_ID`, `User_Skills`, `Job_Requirements` dan `Recommended`. Dalam pembuatan model dengan menggunakan Content Based Filtering fituer yang digunakan adalah `User_Skills` dan `Job_Recommended`. Sementara itu untuk model menggunakan Collaboratorive Filtering fitur yang digunakan adalah `User_ID`, `Job_ID` dan `Recommended`.

## c. Visualisasi Unvariate - EDA

Visualisasi Kolom menggunakan Boxplot
"""

# Tentukan fitur-fitur yang ingin ditampilkan
fitur = ['User_ID', 'Job_ID', 'Match_Score', 'Recommended',
         'User_Skills', 'Job_Requirements']
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for i, fitur_name in enumerate(fitur):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    if fitur_name in ['User_Skills', 'Job_Requirements']:

        skill_lengths = df[fitur_name].str.len()
        sns.boxplot(y=skill_lengths, ax=ax)
        ax.set_ylabel('Panjang Skill')
    else:
        sns.boxplot(data=df, y=fitur_name, ax=ax)
        ax.set_ylabel(fitur_name)

    ax.set_title(f'Box Plot {fitur_name}')

plt.tight_layout()
plt.show()

"""Visualisasi Histogram"""

# Tentukan fitur-fitur yang ingin ditampilkan
fitur = ['User_ID', 'Job_ID', 'Match_Score', 'Recommended',
         'User_Skills', 'Job_Requirements']
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

for i, fitur_name in enumerate(fitur):
    row = i // 3
    col = i % 3
    ax = axes[row, col]

    if fitur_name in ['User_Skills', 'Job_Requirements']:

        skill_lengths = df[fitur_name].str.len()
        sns.histplot(x=skill_lengths, ax=ax)
        ax.set_ylabel('Panjang Skill')
    else:
        sns.histplot(data=df, x=fitur_name, ax=ax)
        ax.set_ylabel(fitur_name)

    ax.set_title(f'Box Plot {fitur_name}')

plt.tight_layout()
plt.show()

"""## c. Visualisasi Multivariate - EDA

Visualisasi Matriks Korelasi
"""

# Pilih kolom numerik yang ingin divisualisasikan
numeric_cols = ['Match_Score', 'Recommended', 'User_ID', 'Job_ID']

# Hitung matriks korelasi
corr_matrix = df[numeric_cols].corr()

# Buat heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Korelasi Antar Variabel')
plt.show()

"""Visualisasi Pairplot"""

sns.pairplot(df[numeric_cols])
plt.show()

def plot_top_values(df, column_name, top_n=10):
    """
    Menampilkan dan memplot nilai-nilai teratas (top_n) yang paling sering muncul
    dalam kolom object DataFrame.

    Args:
        df: pandas DataFrame.
        column_name: Nama kolom (object type).
        top_n: Jumlah nilai teratas yang akan ditampilkan (default: 10).
    """
    all_values = []
    for values in df[column_name]:
        all_values.extend(values.split(','))

    value_counts = Counter(all_values)
    top_values = value_counts.most_common(top_n)

    print(f"Top {top_n} nilai yang sering muncul di kolom '{column_name}':")
    for value, count in top_values:
        print(f"- {value}: {count}")

    colors = ['#87255B', '#56CBF9', '#F5D491', '#BEB7A4', '#B4E1FF', '#F06C9B', '#D3C4D1', '#81F4E1', '#C2AFF0', '#C57B57']

    plt.figure(figsize=(10, 6))
    plt.bar([value[0] for value in top_values], [value[1] for value in top_values], color=colors)
    plt.title(f'Top {top_n} Nilai di Kolom {column_name}')
    plt.xlabel('Nilai')
    plt.ylabel('Frekuensi')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

plot_top_values(df, 'User_Skills')
plot_top_values(df, 'Job_Requirements')

"""# Data Preparation

Penanganan Outliers
"""

filter = ~((df.select_dtypes(include=np.number) < (Q1 - 1.5 * IQR)) | (df.select_dtypes(include=np.number) > (Q3 + 1.5 * IQR))).any(axis=1)

df = df[filter]

df.shape

"""Dataset setelah mengatasi outliers"""

for i, (nama_kolom, jumlah) in enumerate(df.count().items(), 1):
  print(f'Kolom {i}: {nama_kolom} - Berjumlah: {jumlah}')

df

# Mengonversi keterampilan menjadi set
df['User_Skills_Set'] = df['User_Skills'].apply(lambda x: set(x.split(', ')))
df['Job_Requirements_Set'] = df['Job_Requirements'].apply(lambda x: set(x.split(', ')))

"""#### TF-IDF"""

# Pastikan jumlah User dan Job seimbang untuk contoh ini (bisa disesuaikan)
user_df = df[['User_ID', 'User_Skills']].drop_duplicates().reset_index(drop=True)
job_df = df[['Job_ID', 'Job_Requirements']].drop_duplicates().reset_index(drop=True)

# Gabungkan untuk fitting TF-IDF
combined_texts = pd.concat([user_df['User_Skills'], job_df['Job_Requirements']])

# TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=100, max_df=0.8)
vectorizer.fit(df['Job_Requirements_Clean'])

user_vecs = vectorizer.transform(df['User_Skills_Clean'])
job_vecs = vectorizer.transform(df['Job_Requirements_Clean'])

user_vecs[0]

"""#### Preprocessing Collaborative Filtering"""

# Buat instance LightFM Dataset
dataset = Dataset()

# Fit user dan job IDs
dataset.fit(df['User_ID'], df['Job_ID'])

# Buat (user_id, job_id, interaction) tuples
interactions = list(zip(df['User_ID'], df['Job_ID']))
interactions_labels = df['Recommended'].astype(float)

# Build interaction matrix
(interaction_matrix, _) = dataset.build_interactions(zip(df['User_ID'], df['Job_ID'], interactions_labels))

"""### Splitting Data"""

train_interactions, test_interactions = random_train_test_split(interaction_matrix, test_percentage=0.2)

"""# Model Development

## Model Content Based Filtering (User_Skills dan Job Requirements)

### Model NearestNeighbors
"""

# fit pada job vectors
nn = NearestNeighbors(n_neighbors=5, metric='cosine')
nn.fit(job_vecs)

# cari top-3 pekerjaan untuk setiap user
distances, indices = nn.kneighbors(user_vecs)

recommendations = []
for user_idx, job_indices in enumerate(indices):
    user_id = user_df.loc[user_idx, 'User_ID']
    for rank, job_idx in enumerate(job_indices):
        try:
            job_id = df['Job_ID'].iloc[job_idx]
        except IndexError:
            print(f"IndexError: job_idx {job_idx} is out of range for df with length {len(df)}")
            continue

        score = 1 - distances[user_idx][rank]
        recommendations.append({
            'User_ID': user_id,
            'Job_ID': job_id,
            'Similarity_Score': round(score, 4)
        })

recommend_df = pd.DataFrame(recommendations)
print(recommend_df.head(5))

"""Melihat Similarity pada setiap user setiap user dimana diberikan 2 rekomendasi untuk setiap user"""

# Misalnya melihat rekomendasi untuk user dengan ID 10
target_user_id = 10

user_recommendations = recommend_df[recommend_df['User_ID'] == target_user_id]

print(user_recommendations)

"""Sistem telah berhasil merekomendasikan top 5 pekerjaan yang mirip dengan kebutuhan user disesuaikan dengan beberapa skills yang dimiliki oleh user tersebut.

## Model Collaborative Filtering
"""

model = LightFM(loss='warp')
model.fit(train_interactions, epochs=20, num_threads=2)

n_users, n_items = interaction_matrix.shape

user_id = 2000
scores = model.predict(user_id, np.arange(n_items))
top_jobs = np.argsort(-scores)[:5]

job_id_map, job_id_rev_map, _ = dataset.mapping()[1], {v: k for k, v in dataset.mapping()[1].items()}, _
top_job_ids = [job_id_rev_map[i] for i in top_jobs]

print(f"Top 5 rekomendasi pekerjaan untuk User {user_id}:\n")

for job_id in top_job_ids:
    if df[df['Job_ID'] == job_id].empty:
        print(f"Job_ID {job_id} not found in the original DataFrame.")
        continue

    job_info = df[df['Job_ID'] == job_id].iloc[0]
    job_info
    print(f"- Job_ID: {job_info['Job_ID']}")
    print(f"  Requirements: {job_info['Job_Requirements']}")
    print(f"  User Skills: {job_info['User_Skills']}\n")

"""Sistem telah menampilkan 5 rekomendasi pekerjaan untuk user yang dimasukkan, dengan beberapa keterangan lainnya yaitu `Job_Requirements` dan `User_Skills`

# Evaluation

**ROC-AUC** adalah metrik evaluasi untuk masalah klasifikasi biner yang mengukur kemampuan model dalam membedakan antara dua kelas (dalam kasusmu: pekerjaan yang direkomendasikan 1 dan tidak 0).

ROC adalah kurva yang menunjukkan trade-off antara:
- True Positive Rate (TPR): berapa banyak item positif yang berhasil dikenali (juga disebut Recall)
- False Positive Rate (FPR): berapa banyak item negatif yang salah diklasifikasi sebagai positif

AUC (Area Under Curve) adalah luas di bawah kurva ROC, dengan nilai antara:
- 1.0 = model sempurna
- 0.5 = model tebak-tebakan (random guess)
- < 0.5 = model buruk (prediksi berlawanan)

**Precision@K**

**Precision@K** mengukur seberapa banyak rekomendasi yang relevan di dalam **K rekomendasi teratas** yang diberikan oleh model. Precision menghitung **proporsi item relevan** dalam K item yang diprediksi oleh model.

**Rumus Precision@K:**

$
\text{Precision@K} = \frac{\text{Jumlah item relevan di top-K}}{K}
$

Dimana:
- **Jumlah item relevan di top-K** adalah jumlah item yang relevan dalam K rekomendasi teratas.
- **K** adalah jumlah item teratas yang direkomendasikan.

**Recall@K**

**Recall@K** mengukur seberapa banyak item relevan yang ditemukan di dalam **K rekomendasi teratas** yang diberikan oleh model. Recall menghitung **proporsi item relevan** yang berhasil diprediksi oleh model dari seluruh item relevan yang ada.

**Rumus Recall@K:**

$
\text{Recall@K} = \frac{\text{Jumlah item relevan di top-K}}{\text{Jumlah total item relevan}}
$

Dimana:
- **Jumlah total item relevan** adalah jumlah keseluruhan item relevan yang seharusnya direkomendasikan kepada pengguna.
- **K** adalah jumlah item teratas yang direkomendasikan.
"""

# Precision at k
precision = precision_at_k(model, interaction_matrix, k=5).mean()
print(f"Precision@5: {precision:.4f}")

recall = recall_at_k(model, interaction_matrix, k=5).mean()
print(f"Recall@5: {recall:.4f}")

# ROC AUC Score
auc = auc_score(model, interaction_matrix).mean()
print(f"AUC Score: {auc:.4f}")

"""Terlihat model ini memiliki performa yang baik dikarenakan mendapatkan nilai ROC-AUC karena mendapatkan nilai 0.900"""